{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN Cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_a_qHs1gpjx"
      },
      "source": [
        "[Credit to kaggle user palaksood97](https://www.kaggle.com/palaksood97/cartpole-dqn#Define-agent)\n",
        "\n",
        "My first go at using the OpenAI Gym and writing a DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV0wd-OUiPqd"
      },
      "source": [
        "## Initialize gym environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwQXTr_0cOb_"
      },
      "source": [
        "import random\n",
        "import os\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import gym"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui6H61WJmHIc"
      },
      "source": [
        "def query_environment(name):\n",
        "  env = gym.make(name)\n",
        "  spec = gym.spec(name)\n",
        "  print(f\"Action Space: {env.action_space}\")\n",
        "  print(f\"Observation Space: {env.observation_space}\")\n",
        "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "  print(f\"Reward Range: {env.reward_range}\")\n",
        "  print(f\"Reward Threshold: {spec.reward_threshold}\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV-9clVTmOXX",
        "outputId": "e363dc63-7c63-4bae-9070-80ebc7200808"
      },
      "source": [
        "query_environment('Taxi-v3')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(6)\n",
            "Observation Space: Discrete(500)\n",
            "Max Episode Steps: 200\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfSu5CqccPIP"
      },
      "source": [
        "env = gym.make('Taxi-v3')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPb05u-ki15e"
      },
      "source": [
        "## Random agent to practice interacting with the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGj2M3CAiaoD",
        "outputId": "64909fc4-8a88-41d9-983d-92651fdf8e52"
      },
      "source": [
        "print(\"*****Random Agent*****\")\n",
        "for i_episode in range (2):\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "  for t in range(5):\n",
        "    # env.render()\n",
        "    action = env.action_space.sample() # choose random action: left or right (0 or 1)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    print(state, reward, action)\n",
        "    total_reward += reward \n",
        "    if done:\n",
        "      print(f\"Episode finished after {t} timesteps with total reward {total_reward}\")\n",
        "      break\n",
        "  env.close()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****Random Agent*****\n",
            "303 -1 1\n",
            "403 -1 0\n",
            "403 -10 5\n",
            "403 -1 3\n",
            "403 -10 4\n",
            "488 -1 2\n",
            "388 -1 1\n",
            "288 -1 1\n",
            "288 -10 5\n",
            "188 -1 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG3_tWECoTIr"
      },
      "source": [
        "## DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX_kfWhAjig7"
      },
      "source": [
        "class DQNAgent:\n",
        "\n",
        "  \"\"\"\n",
        "    action_space:     action space of the gym environment\n",
        "    gamma:            discount/decay/gamme\n",
        "    epsilon:          exploration rate\n",
        "    epsilon_decay:    exploration rate decay\n",
        "    epsilon_min:      minimum exploration rate\n",
        "    history_len:      how much past data to store for re-training\n",
        "    batch_size:       batch size for training NN\n",
        "    model:            a keras model defining the NN\n",
        "  \"\"\"\n",
        "  def __init__(self, model, action_space, gamma=.95, epsilon=1, epsilon_decay=.995, epsilon_min=.1, history_len=2000, batch_size=32):\n",
        "    self.action_space = action_space\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon_min = epsilon_min\n",
        "    self.model = model\n",
        "    self.memory = deque(maxlen=history_len)\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  ## remember a state for later re-training\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  # get next action\n",
        "  def get_action(self, state):\n",
        "    # epsilon greedy exploration\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      # explore\n",
        "      return action_space.sample()\n",
        "    actions = self.model.predict((state,1))\n",
        "    return np.argmax(actions[0])\n",
        "\n",
        "  # training loop\n",
        "  def train(self):\n",
        "    batch = random.sample(self.memory, self.batch_size)\n",
        "    for state, action, reward, next_state, done in batch:\n",
        "      target = reward\n",
        "      if not done:\n",
        "        target = (reward + self.gamma * np.amax(self.model.predict((next_state,1))[0]))\n",
        "      target_f = self.model.predict((state,1))\n",
        "      target_f[0][action] = target\n",
        "      self.model.fit(np.array([state, 1]), target_f, epochs=1, verbose=0)\n",
        "      if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "\n",
        "  # load model weights\n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name)\n",
        "\n",
        "  # save model weights\n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name)"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPXKoqedt5jL"
      },
      "source": [
        "def basic_model(action_space, observation_space, learning_rate):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(24, input_shape=(1,), activation='relu'))\n",
        "  model.add(Dense(24, activation='relu'))\n",
        "  model.add(Dense(action_space.n, activation='linear'))\n",
        "  model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
        "  return model"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwHncRS8zc0s"
      },
      "source": [
        "## Train the agent(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF2gHSNE0ANF"
      },
      "source": [
        "observation_space = env.observation_space\n",
        "action_space = env.action_space"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RVclh7MzWGm"
      },
      "source": [
        "model = basic_model(action_space, observation_space, .001)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBOEN1Ip33SA",
        "outputId": "b5a777db-c2cc-4a99-c907-4eb12347d9c6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_36 (Dense)             (None, 24)                48        \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 6)                 150       \n",
            "=================================================================\n",
            "Total params: 798\n",
            "Trainable params: 798\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WMzrktd05RF"
      },
      "source": [
        "agent = DQNAgent(model, action_space)"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqN0yP_I0lmX"
      },
      "source": [
        "num_episodes = 1000"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IOEv4mq0Fgp",
        "outputId": "41dc7f9e-4d6e-4a41-818b-c59e06939104"
      },
      "source": [
        "done = False\n",
        "for episode in range(num_episodes):\n",
        "  state = env.reset()\n",
        "  total_reward = 0\n",
        "  for time in range(5000):\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    agent.remember(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    if done:\n",
        "      print(f\"episode {episode}/{num_episodes} finished with score {reward}\")\n",
        "      break;\n",
        "    if len(agent.memory) >= agent.batch_size:\n",
        "      agent.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode 0/1000 finished with score -10\n",
            "episode 1/1000 finished with score -1\n",
            "episode 2/1000 finished with score -1\n",
            "episode 3/1000 finished with score -1\n",
            "episode 4/1000 finished with score -1\n",
            "episode 5/1000 finished with score -1\n",
            "episode 6/1000 finished with score -1\n",
            "episode 7/1000 finished with score -1\n",
            "episode 8/1000 finished with score -1\n",
            "episode 9/1000 finished with score -1\n",
            "episode 10/1000 finished with score -1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vvLdGAj7RGU"
      },
      "source": [
        ""
      ],
      "execution_count": 167,
      "outputs": []
    }
  ]
}